# Powder Integration 卡顿问题 - 诊断步骤

## 当前状态
已尝试的修复方案都无效：
1. ❌ stdout/stderr 重定向
2. ❌ 禁用 tqdm
3. ❌ QThread -> threading.Thread  
4. ❌ 延迟启动线程

## 可能的根本原因

### 假设 1: pyFAI 导入阻塞
**症状**: 在 `from batch_integration import run_batch_integration` 时卡住
**原因**: pyFAI/fabio 等库的导入可能触发一些初始化操作
**测试方法**: 在启动应用时提前导入 pyFAI

### 假设 2: 日志更新导致卡顿
**症状**: self.log() 调用过于频繁，QTextEdit.append() 阻塞主线程
**原因**: 每次 append 都会触发 QTextEdit 重新渲染
**测试方法**: 完全禁用日志更新

### 假设 3: 进度条动画导致卡顿  
**症状**: self.progress.start() 的 QTimer 与主事件循环冲突
**原因**: QTimer 每 35ms 触发一次，可能与其他事件冲突
**测试方法**: 禁用进度条动画

### 假设 4: 文件 I/O 阻塞
**症状**: glob.glob() 或文件打开操作在主线程执行
**原因**: 大量文件或网络驱动器导致文件查找很慢
**测试方法**: 先检查文件数量

## 诊断方案

### 方案 A: 最小化测试
运行 `test_threading.py` 验证基础线程机制是否正常

```bash
python3 test_threading.py
```

如果测试脚本也卡，说明是 PyQt6 环境问题
如果测试脚本正常，继续下一步

### 方案 B: 逐步排除
修改 powder_module.py，在 run_task() 中只执行最简单的操作：

```python
def run_task():
    import time
    time.sleep(3)  # 只是等待3秒
    # 不调用任何 batch_integration 函数
```

如果这样还卡，说明问题在线程创建/信号机制
如果不卡，说明问题在 batch_integration.py

### 方案 C: 检查文件数量
在启动集成前，先检查匹配的文件数量：

```python
import glob
files = glob.glob(self.input_pattern)
print(f"找到 {len(files)} 个文件")
```

如果文件数量巨大（> 1000），可能是文件系统瓶颈

### 方案 D: 禁用所有 UI 更新
修改 run_integration()：

```python
# 注释掉所有 self.log() 调用
# 注释掉 self.progress.start()
# 只启动线程
```

如果不卡，说明是 UI 更新导致的

## 建议的修复方向

### 如果是日志问题
**解决方案**: 使用队列批量更新日志

```python
from queue import Queue

class PowderXRDModule:
    def __init__(self):
        self.log_queue = Queue()
        self.log_timer = QTimer()
        self.log_timer.timeout.connect(self._flush_logs)
        self.log_timer.start(100)  # 每100ms更新一次
    
    def log(self, message):
        self.log_queue.put(message)
    
    def _flush_logs(self):
        while not self.log_queue.empty():
            msg = self.log_queue.get()
            self.log_text.append(msg)
```

### 如果是 pyFAI 导入问题
**解决方案**: 在应用启动时预先导入

在 main.py 开头添加：
```python
# 预加载重量级库
try:
    import pyFAI
    import fabio
except:
    pass
```

### 如果是文件 I/O 问题
**解决方案**: 将文件查找也放到后台线程

```python
def run_task():
    # 在后台线程中执行文件查找
    files = glob.glob(input_pattern)
    # 然后再执行集成
    ...
```

### 如果是进度条问题
**解决方案**: 使用更简单的进度指示器

```python
# 用简单的文本替代动画
self.status_label.setText("Processing...")
```

## 快速测试命令

### 测试 1: 验证线程机制
```bash
python3 test_threading.py
```

### 测试 2: 检查文件数量
```python
import glob
pattern = "你的输入模式"
files = glob.glob(pattern)
print(f"文件数量: {len(files)}")
```

### 测试 3: 验证 pyFAI 导入速度
```python
import time
start = time.time()
import pyFAI
import fabio
end = time.time()
print(f"导入耗时: {end - start:.2f} 秒")
```

## 下一步行动

1. **首先**: 运行 test_threading.py 确认基础机制
2. **然后**: 根据测试结果选择对应的修复方案
3. **最后**: 逐步恢复功能，找到真正的瓶颈

请告诉我：
- test_threading.py 是否也会卡？
- 输入文件有多少个？
- 卡住时CPU使用率如何？
- 卡住时可以拖动窗口吗？
